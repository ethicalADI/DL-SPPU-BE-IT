{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4317279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "96b4ed3b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 62\n"
     ]
    }
   ],
   "source": [
    "# Data: list of sentences about deep learning\n",
    "data = [\n",
    "    \"Deep learning also known as deep structured learning\",\n",
    "    \"is part of a broader family of machine learning methods based\",\n",
    "    \"on artificial neural networks with representation learning\",\n",
    "    \"Learning can be supervised, semi-supervised or unsupervised\",\n",
    "    \"Deep-learning architectures such as deep neural networks\",\n",
    "    \"deep belief networks, deep reinforcement learning\",\n",
    "    \"recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation\",\n",
    "    \"where they have produced results comparable to and in some cases surpassing human expert performance\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences and build vocabulary\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "word2id = tokenizer.word_index\n",
    "word2id['PAD'] = 0  # Add padding for sequence compatibility\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "# Convert sentences to sequences of word IDs\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in data]\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100  # Embedding vector size\n",
    "window_size = 2  # Context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58049628",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Context-Target Pairs:\n",
      "Context (X): ['learning', 'also'] -> Target (Y): deep\n",
      "Context (X): ['deep', 'also', 'known'] -> Target (Y): learning\n",
      "Context (X): ['deep', 'learning', 'known', 'as'] -> Target (Y): also\n",
      "Context (X): ['learning', 'also', 'as', 'deep'] -> Target (Y): known\n",
      "Context (X): ['also', 'known', 'deep', 'structured'] -> Target (Y): as\n",
      "Context (X): ['known', 'as', 'structured', 'learning'] -> Target (Y): deep\n"
     ]
    }
   ],
   "source": [
    "# Function to generate context-target pairs\n",
    "\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            # Get context words\n",
    "            context_words = [\n",
    "                words[i]\n",
    "                for i in range(index - window_size, index + window_size + 1)\n",
    "                if 0 <= i < sentence_length and i != index\n",
    "            ]\n",
    "            # Pad context to fixed length\n",
    "            x = pad_sequences([context_words], maxlen=context_length)[0]\n",
    "            y = to_categorical(word, vocab_size)\n",
    "            yield (x, y)\n",
    "\n",
    "# Display a few context-target pairs\n",
    "print(\"\\nSample Context-Target Pairs:\")\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(wids, window_size, vocab_size):\n",
    "    print('Context (X):', [id2word[w] for w in x if w != 0], '-> Target (Y):', id2word[np.argmax(y)])\n",
    "    if i == 5:  # Show only the first 5 pairs\n",
    "        break\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43bbd775",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 4, 100)            6200      \n",
      "                                                                 \n",
      " lambda_8 (Lambda)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 62)                6262      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12462 (48.68 KB)\n",
      "Trainable params: 12462 (48.68 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the CBOW model\n",
    "cbow = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size * 2),\n",
    "    Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(cbow.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "396a5163",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t  Loss: 300.8581107854843\n",
      "Epoch: 20 \t  Loss: 249.75314646959305\n",
      "Epoch: 30 \t  Loss: 210.08205100893974\n",
      "Epoch: 40 \t  Loss: 178.21800927817822\n",
      "Epoch: 50 \t  Loss: 150.4565201178193\n"
     ]
    }
   ],
   "source": [
    "# Train the CBOW model\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = 0\n",
    "    for x, y in generate_context_word_pairs(wids, window_size, vocab_size):\n",
    "        loss += cbow.train_on_batch(x.reshape(1, -1), y.reshape(1, -1))\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:', epoch, '\\t  Loss:', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b577304",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding matrix shape: (61, 100)\n",
      "\n",
      "Word Embeddings (Sample):\n",
      "                0         1         2         3         4         5   \\\n",
      "learning  0.160850 -0.500997  0.247963  0.258451 -0.310178  0.255225   \n",
      "deep      0.043551  0.298463  0.269905 -0.181816 -0.095490 -0.017680   \n",
      "networks  0.168837 -0.390784 -0.777694  0.566408 -0.159862  0.279692   \n",
      "neural    0.609843 -0.749018  0.272168  0.025571 -0.482311 -0.356484   \n",
      "as        0.164156 -0.149009  0.076493  0.308174  0.225752  0.388742   \n",
      "\n",
      "                6         7         8         9   ...        90        91  \\\n",
      "learning -0.027897  0.511773  0.229951  0.232851  ... -0.252065  0.040186   \n",
      "deep      0.204720 -0.239713  0.086499 -0.344364  ... -0.245923 -0.075849   \n",
      "networks  0.005954  0.438587  0.057010  0.132329  ... -0.289924 -0.543459   \n",
      "neural   -0.268259 -0.050787  0.316267 -0.479768  ... -0.078136 -0.605091   \n",
      "as        0.024141  0.322571  0.136768  0.215855  ... -0.409961  0.249628   \n",
      "\n",
      "                92        93        94        95        96        97  \\\n",
      "learning -0.375583  0.196460  0.289114 -0.207839 -0.509605  0.697973   \n",
      "deep      0.083116  0.115091 -0.810800 -0.109902 -0.252409 -0.068009   \n",
      "networks -0.401716 -0.265122  0.265818  0.124941  0.107755  0.642649   \n",
      "neural   -0.607208  0.497264 -0.284073  0.229347 -0.018525 -0.287916   \n",
      "as       -0.357766 -0.272119 -0.314115 -0.098435 -0.288708  0.445806   \n",
      "\n",
      "                98        99  \n",
      "learning -0.460731  0.048941  \n",
      "deep     -0.131653  0.257348  \n",
      "networks  0.621006  0.245976  \n",
      "neural    0.287716  0.687601  \n",
      "as       -0.228887  0.174777  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract the learned word embeddings\n",
    "embedding_weights = cbow.get_weights()[0][1:]  # Exclude the padding index\n",
    "print(\"\\nEmbedding matrix shape:\", embedding_weights.shape)\n",
    "\n",
    "# Display word embeddings as a DataFrame\n",
    "print(\"\\nWord Embeddings (Sample):\")\n",
    "embedding_df = pd.DataFrame(embedding_weights, index=[id2word[i] for i in range(1, vocab_size)])\n",
    "print(embedding_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12bc3dda",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distance matrix shape: (61, 61)\n",
      "\n",
      "Similar Words: {'deep': ['representation', 'with', 'known', 'recurrent', 'convolutional'], 'unsupervised': ['semi', 'or', 'can', 'be', 'on']}\n"
     ]
    }
   ],
   "source": [
    "# Compute pairwise Euclidean distances between embeddings\n",
    "distance_matrix = euclidean_distances(embedding_weights)\n",
    "print(\"\\nDistance matrix shape:\", distance_matrix.shape)\n",
    "\n",
    "# Find and display similar words for specific search terms\n",
    "similar_words = {\n",
    "    search_term: [\n",
    "        id2word[idx + 1] for idx in distance_matrix[word2id[search_term] - 1].argsort()[1:6]\n",
    "    ]\n",
    "    for search_term in ['deep', 'unsupervised']\n",
    "}\n",
    "print(\"\\nSimilar Words:\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdab992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b544b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
